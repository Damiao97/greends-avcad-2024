{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization of Complex Agro-Environmental Data\n",
    "---\n",
    "### Exercise #10 - correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 10.1 - Run a PCA based on quantitative environmental variables (see previous exercise), using sites from the Douro and Tejo basins. Produce a biplot with ‘Catchment_name’ as a grouping variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('EFIplus_medit.zip',compression='zip', sep=\";\")\n",
    "df = df.dropna() # remove all rows with missing data\n",
    "# Subset the df by selecting the environmental variables and the species richness columns\n",
    "dfsub = df[(df['Catchment_name']=='Douro') | (df['Catchment_name']=='Tejo')]\n",
    "df_env = dfsub[[\"Altitude\", \"Actual_river_slope\",\"Elevation_mean_catch\", \"prec_ann_catch\",\"temp_ann\",\"temp_jan\",\"temp_jul\"]]\n",
    "df_catch = dfsub[[\"Catchment_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efi_scaled = StandardScaler().fit_transform(df_env)\n",
    "# As a result, we obtained a two-dimensional NumPy array. We can convert it to a pandas DataFrame for a better display.\n",
    "df_scaled = pd.DataFrame(data=efi_scaled, \n",
    "                                columns=df_env.columns)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select number of PCs\n",
    "pca = PCA(n_components=7)\n",
    "pca.fit_transform(df_scaled)\n",
    "eigenvalues = pca.explained_variance_ # eigenvalues\n",
    "prop_var = pca.explained_variance_ratio_ # proportion of explained variance\n",
    "\n",
    "# Scree Plot\n",
    "PC_numbers = np.arange(pca.n_components_) + 1\n",
    " \n",
    "plt.plot(PC_numbers, \n",
    "         prop_var,\n",
    "         'ro-')\n",
    "plt.title('Scree Plot', fontsize=20)\n",
    "plt.ylabel('Proportion of Variance', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "PC = pca.fit_transform(df_scaled)\n",
    "pca_efi = pd.DataFrame(data = PC, \n",
    "                            columns = ['PC1', 'PC2'])\n",
    "pca_efi.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot\n",
    "PC1 = pca_efi['PC1']/(pca_efi['PC1'].max() - pca_efi['PC1'].min())\n",
    "PC2 = pca_efi['PC2']/(pca_efi['PC2'].max() - pca_efi['PC2'].min())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Biplot of PCA')\n",
    "sns.scatterplot(x=PC1,\n",
    "              y=PC2,\n",
    "              hue = df_catch['Catchment_name'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "\n",
    "n = np.transpose(pca.components_).shape[0] # number of dimensions (2)\n",
    "for i in range(n):\n",
    "        plt.arrow(0, 0, np.transpose(pca.components_)[i,0], \n",
    "                  np.transpose(pca.components_)[i,1], \n",
    "                  color = (0.1, 0.1, 0.1, 0.8),\n",
    "                  head_width=0.02) # plot arrows for each variable\n",
    "        plt.text(np.transpose(pca.components_)[i,0]* 1.15, \n",
    "                 np.transpose(pca.components_)[i,1] * 1.15, \n",
    "                 list(df_env.columns)[i], \n",
    "                 color = (0.1, 0.1, 0.1, 0.8), \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    "plt.legend(title='Catchment name')\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 10.2 - Using the same data, run a PCoA and project the sites using the resulting first two axis. Use also the ‘Catchment_name’ as a grouping variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MDS\n",
    "# by default the 'mds' argument is set to 'True', which means it will run a MDS\n",
    "# Euclidean distances are also the default \n",
    "# Only two axis are extracted by default\n",
    "mds = MDS(random_state=0, normalized_stress = False) \n",
    "mds_transf = mds.fit_transform(df_scaled)\n",
    "# plot the MDS\n",
    "sns.scatterplot(x=mds_transf[:,0],\n",
    "              y=mds_transf[:,1],\n",
    "              hue = df_catch['Catchment_name'].tolist(),\n",
    "              linewidth=0,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMDS\n",
    "nmds = MDS(n_components=5, metric = False, normalized_stress=\"auto\") # 5 components extracted so that stress is > 0.2\n",
    "nmds_transf = nmds.fit_transform(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress = nmds.stress_\n",
    "print(stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=nmds_transf[:,0],\n",
    "              y=nmds_transf[:,1],\n",
    "              hue = df_catch['Catchment_name'].tolist(),\n",
    "              linewidth=0,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 10.3 - . Run a Linear Discriminant Analysis based on quantitative environmental variables (see previous exercises), using the Douro and Tejo basins as the pre-determined groups. Produce a biplot with ‘Catchment_name’ as a grouping variable. \n",
    "\n",
    "NOTE: to be more interesting it is better to consider more catchments, so the Minho and Mondego catchments were also considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('EFIplus_medit.zip',compression='zip', sep=\";\")\n",
    "df = df.dropna() # remove all rows with missing data\n",
    "# Subset the df by selecting the environmental variables and the species richness columns\n",
    "dfsub = df[(df['Catchment_name']=='Douro') | (df['Catchment_name']=='Tejo') | (df['Catchment_name']=='Minho') | (df['Catchment_name']=='Mondego')]\n",
    "df_env = dfsub[[\"Altitude\", \"Actual_river_slope\",\"Elevation_mean_catch\", \"prec_ann_catch\",\"temp_ann\",\"temp_jan\",\"temp_jul\"]]\n",
    "df_catch = dfsub[[\"Catchment_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efi_scaled = StandardScaler().fit_transform(df_env)\n",
    "# As a result, we obtained a two-dimensional NumPy array. We can convert it to a pandas DataFrame for a better display.\n",
    "df_scaled = pd.DataFrame(data=efi_scaled, \n",
    "                                columns=df_env.columns)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define predictor and response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_scaled\n",
    "y = dfsub['Catchment_name'] # 4 classes (Tagus, Douro and Minho and Mondego)\n",
    "\n",
    "#Fit the LDA model\n",
    "model = LinearDiscriminantAnalysis(n_components=2)\n",
    "DLA = model.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the first two discriminant axis to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DLA_scores = pd.DataFrame(data = DLA, \n",
    "                            columns = ['LD1', 'LD2'])\n",
    "DLA_scores.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define method to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the kfold crossvalidation settings for the next function 'cross_val_score'\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "\n",
    "#evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fist discriminant plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.scatterplot(x=DLA_scores['LD1'],\n",
    "              y=DLA_scores['LD2'],\n",
    "              hue = dfsub['Catchment_name'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "\n",
    "n = model.n_features_in_\n",
    "for i in range(n):\n",
    "        plt.arrow(0, 0, model.scalings_[i,0]*2, # Scalings were multiplied by a factor of 4 to just to facilitate the visualization\n",
    "                  model.scalings_[i,1]*2, # Scalings were multiplied by a factor of 4 to just to facilitate the visualization\n",
    "                  color = (0.1, 0.1, 0.1, 0.8),\n",
    "                  head_width=0.02) # plot arrows for each variable\n",
    "        plt.text(model.scalings_[i,0]*2.1, # plot the names of the variables\n",
    "                 model.scalings_[i,1]*2.1,\n",
    "                 list(df_scaled)[i], \n",
    "                 color = (0.1, 0.1, 0.1, 0.8), \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    "\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
